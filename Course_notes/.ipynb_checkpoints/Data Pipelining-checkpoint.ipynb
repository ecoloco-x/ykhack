{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAS5MqM9xy-N"
   },
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data pipeline is a series of processes that move data from one place to another. In Python, a data pipeline can be built using various libraries and tools such as pandas, numpy, scikit-learn, etc.\n",
    "\n",
    "For example, you can use pandas to extract data from a database, clean and transform the data, and then use scikit-learn to train a machine learning model on the transformed data. Finally, you can use the trained model to make predictions on new data. This series of steps constitutes a data pipeline in Python.\n",
    "\n",
    "By building a data pipeline, you can automate many of the repetitive and time-consuming tasks involved in working with data, such as data loading, cleaning, and transformation, making the process more efficient and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1623063461098,
     "user": {
      "displayName": "Ignacio Soteras",
      "photoUrl": "",
      "userId": "02050793736257155229"
     },
     "user_tz": -120
    },
    "id": "DVtwE6bRxy-V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 50\n",
    "from scipy.stats import scoreatpercentile as pct,iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_original_data():\n",
    "    file1 = pd.read_csv('Data/file1.csv')\n",
    "    file2 = pd.read_csv('Data/file2.txt', sep = '\\t')\n",
    "    file3 = pd.read_excel('Data/file3.xlsx')\n",
    "    file4 = pd.read_excel('Data/file4.xlsx')\n",
    "    return pd.concat([file1,file2,file3, file4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_in_col(df,col):\n",
    "    col = str(col)\n",
    "    print(\"removing outliers in columns: \", col)\n",
    "    pct_75 = pct(df[col], 75)  # Calculate percentile 75 using scipy function scoreatpercentile\n",
    "    pct_25 = pct(df[col], 25)  # Calculate percentile 25 using scipy function scoreatpercentile\n",
    "    upper_bound = pct_75 + 1.5*iqr(df[col])  # iqr - > Scipy function to calculate the Interquartile Range\n",
    "    lower_bound = pct_25 - 1.5*iqr(df[col])\n",
    "    return df[(df[col] <= upper_bound) & (df[col] >= lower_bound)][col]  # Filter out the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    for col in list(df._get_numeric_data().columns):\n",
    "        df[col] = remove_outliers_in_col(df,col).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_column_names(hk_df):\n",
    "    hk_df.columns=[i.lower() for i in hk_df.columns]\n",
    "    return hk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(hk_df):\n",
    "    hk_df.rename(columns={'controln':'id','hv1':'median_home_val', 'ic1':'median_household_income',\"ic2\":\"med_fam_income\", \"ic3\":\"avg_household_income\",\"ic4\": \"avg_fam_income\",\"ic5\":\"per_capita_income\"}, inplace=True )\n",
    "    return hk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(hk_df) :\n",
    "    hk_df.drop(columns=[\"id\",\"tcode\",'pobc1',\"dob\"], inplace=True)\n",
    "    return hk_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gender(hk_df):\n",
    "    ## do your magic about gender cleanfing\n",
    "    hk_df[\"gender\"]\n",
    "    return hk_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(hk_df._get_numeric_data().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing outliers in columns:  median_household_income\n",
      "removing outliers in columns:  avg_fam_income\n",
      "removing outliers in columns:  hvp1\n",
      "removing outliers in columns:  pobc2\n",
      "removing outliers in columns:  med_fam_income\n",
      "removing outliers in columns:  avg_household_income\n",
      "removing outliers in columns:  avggift\n",
      "removing outliers in columns:  target_d\n"
     ]
    }
   ],
   "source": [
    "hk_df = load_original_data()\n",
    "\n",
    "hk_df= hk_df.pipe(lower_case_column_names).pipe(rename_columns).pipe(drop_columns).pipe(clean_gender).pipe(remove_outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining using Sklearn - Another Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, the pipeline first applies the SimpleImputer to fill missing values with the mean value, then applies the StandardScaler to scale the data. The resulting cleaned and transformed data is stored in the df_cleaned variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Define a pipeline with steps for imputing missing values and scaling the data\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the DataFrame and transform the data\n",
    "df_cleaned = pipeline.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the pipeline as python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written the code to produce this analysis once, we will want to automate it so that we can run it again next year without having to modify the code. To do this, we are going to organize our code by wrapping it into functions according to the step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 50\n",
    "from scipy.stats import scoreatpercentile as pct,iqr\n",
    "\n",
    "def load_original_data():\n",
    "    file1 = pd.read_csv('Data/file1.csv')\n",
    "    file2 = pd.read_csv('Data/file2.txt', sep = '\\t')\n",
    "    file3 = pd.read_excel('Data/file3.xlsx')\n",
    "    file4 = pd.read_excel('Data/file4.xlsx')\n",
    "    return pd.concat([file1,file2,file3, file4], axis=0)\n",
    "\n",
    "def remove_outliers_in_col(df,col):\n",
    "    col = str(col)\n",
    "    print(\"removing outliers in columns: \", col)\n",
    "    pct_75 = pct(df[col], 75)  # Calculate percentile 75 using scipy function scoreatpercentile\n",
    "    pct_25 = pct(df[col], 25)  # Calculate percentile 25 using scipy function scoreatpercentile\n",
    "    upper_bound = pct_75 + 1.5*iqr(df[col])  # iqr - > Scipy function to calculate the Interquartile Range\n",
    "    lower_bound = pct_25 - 1.5*iqr(df[col])\n",
    "    return df[(df[col] <= upper_bound) & (df[col] >= lower_bound)][col]  # Filter out the outliers\n",
    "\n",
    "def remove_outliers(df):\n",
    "    for col in list(df._get_numeric_data().columns):\n",
    "        df[col] = remove_outliers_in_col(df,col).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def lower_case_column_names(hk_df):\n",
    "    hk_df.columns=[i.lower() for i in hk_df.columns]\n",
    "    return hk_df\n",
    "\n",
    "def rename_columns(hk_df):\n",
    "    hk_df.rename(columns={'controln':'id','hv1':'median_home_val', 'ic1':'median_household_income',\"ic2\":\"med_fam_income\", \"ic3\":\"avg_household_income\",\"ic4\": \"avg_fam_income\",\"ic5\":\"per_capita_income\"}, inplace=True )\n",
    "    return hk_df\n",
    "\n",
    "def drop_columns(hk_df) :\n",
    "    hk_df.drop(columns=[\"id\",\"tcode\",'pobc1',\"dob\"], inplace=True)\n",
    "    return hk_df\n",
    "\n",
    "def clean_gender(hk_df):\n",
    "    ## do your magic about gender cleanfing\n",
    "    hk_df[\"gender\"]\n",
    "    return hk_df\n",
    "\n",
    "## Executing the main program\n",
    "if __name__ == '__main__':\n",
    "    hk_df = load_original_data()\n",
    "    hk_df= hk_df.pipe(lower_case_column_names).pipe(rename_columns).pipe(drop_columns).pipe(clean_gender).pipe(remove_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have also added an if __name__ == '__main__' statement at the bottom where the functions are actually called. This statement should be added when there is code that we want the Python interpreter to execute only when a script is run as the main program (not imported into another Python file).\n",
    "\n",
    "We are going to save this code in a Python file called health_care_data_cleaning_pipeline.py file that we can then run via the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your pipeline does not need any user intervention, you can schedule it to run automatically at a desired frequency (daily, weekly, monthly, annually, etc.). The instructions for how to do that vary significantly from one operating system to another, depend on where on your machine you installed Python, and is ultimately beyond the scope of this lesson. However, scheduling is something that you should research for the operating system you have.\n",
    "\n",
    "For Mac users, you want to Google cron and launchd and look for tutorials. Windows users will want to investigate the Task Scheduler that comes with Windows. Both operating systems make it possible to run Python scripts automatically on a scheduler, but neither of them has a way to do so that is both \"official\" and easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Intro to Pandas - \"Healthcare for All\" Code Along - Structure.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
